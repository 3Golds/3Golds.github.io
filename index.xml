<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>赵鑫的博客</title>
    <link>http://zhaox.xyz/</link>
    <description>Recent content on 赵鑫的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 12 Aug 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://zhaox.xyz/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>About</title>
      <link>http://zhaox.xyz/about/</link>
      <pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/about/</guid>
      <description> 关于我  邮箱: im.xindi@gmail.com Github: 3Golds  </description>
    </item>
    
    <item>
      <title>Golang 常量</title>
      <link>http://zhaox.xyz/posts/golang-%E5%B8%B8%E9%87%8F/</link>
      <pubDate>Thu, 14 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/golang-%E5%B8%B8%E9%87%8F/</guid>
      <description> 在 Go 语言中，常量是指编译期间就已知且不可改变的值。常量可以是数值类型(包括整数、浮点数和负数类型)、布尔类型、字符串类型等。
常量定义 通过 const 关键字定义，有以下三种方式
 const v int = 8989 const fu = 0.2 const ( long int64 = 2048 size = -1 )  预定义常量 Go 语言预定义了一些常量：true、false 和 iota。其中 iota 比较特殊，可以被认为是一个可被编译器修改的常量，在第一个 const 关键字出现时被重置为 0，然后在下一个 const 出现之前，每出现一次 iota，其所代表的数字会自动增 1，我们看一个例子。
 const ( c0 = iota // iota被重设为0 c1 = iota // c1 = 1 c2 // c2 = 2 )  </description>
    </item>
    
    <item>
      <title>Golang 流程控制语句(for、if、else、switch)</title>
      <link>http://zhaox.xyz/posts/golang%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6%E8%AF%AD%E5%8F%A5-for-if-else-switch/</link>
      <pubDate>Thu, 14 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/golang%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6%E8%AF%AD%E5%8F%A5-for-if-else-switch/</guid>
      <description>for 循环 在 Go 中，只有一种循环结构 &amp;mdash;&amp;gt; for循环
基本的for循环包含三个由分号分开的组成部分：
 初始化语句：在第一次循环执行前被执行
 循环条件表达式：每轮迭代开始前被求值
 后置语句：每轮迭代后被执行
  初始化语句一般是一个短变量声明，这里声明的变量仅在整个for循环语句可见。
如果条件表达式的值变为false，那么迭代将终止。
注意：不像 C，Java，或者 Javascript 等其他语言，for 语句的三个组成部分 并不需要用括号括起来，但循环体必须用 { } 括起来。
package main import &amp;quot;fmt&amp;quot; func main() { sum := 0 for i := 0; i &amp;lt; 10; i++ { sum += i } fmt.Println(sum) }  for 的另一种格式 循环初始化语句和后置语句都是可选的
package main import &amp;quot;fmt&amp;quot; func main() { sum := 1 for ; sum &amp;lt; 1000; { sum += sum } fmt.</description>
    </item>
    
    <item>
      <title>Golang 常量</title>
      <link>http://zhaox.xyz/posts/%E5%88%9D%E8%AF%86golang/</link>
      <pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/%E5%88%9D%E8%AF%86golang/</guid>
      <description>Go 介绍 Go 是一种开源编程语言，可以轻松构建简单，可靠和高效的软件。
Go 是 Google 开发的一种静态强类型、编译型、并发型，并具有垃圾回收功能的编程语言。为了方便搜索和识别，有时会将其称为 Golang。
罗伯特·格瑞史莫，罗勃·派克（Rob Pike）及肯·汤普逊于 2007 年 9 月开始设计 Go 语言，稍后 Ian Lance Taylor、Russ Cox 加入项目。Go 语言是基于 Inferno 操作系统所开发的。Go 语言于 2009 年 11 月正式宣布推出，成为开放源代码项目，并在 Linux 及 Mac OS X 平台上进行了实现，后来追加了 Windows 系统下的实现。
受欢迎程度 截止目前为止，越来越多的开发者开始使用 GO，并喜欢用 Go，目前在 Github 统计中，Go 排名第 9
最受欢迎的 5 种语言和最想使用的语言之一 来源: https://insights.stackoverflow.com/survey/2017#most-loved-dreaded-and-wanted
Go 是云基础架构语言 每个云计算公司都在用 Go 实现云基础架构关键组件，包括 Google Cloud, AWS, Microsoft Azure, DigitalOcean, Heroku。Go 是阿里巴巴，cloudflare 和 Drobox 等云计算公司的重要组成部分
Go 开源软件的影响力 Go 有很多出色的开源软件，并且很流行，举几个常见并且正在用的例子</description>
    </item>
    
    <item>
      <title>Opentracing Zipkin</title>
      <link>http://zhaox.xyz/posts/2017-11-09-opentracing-zipkin/</link>
      <pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-11-09-opentracing-zipkin/</guid>
      <description>Zipkin 是一个开源分布式的追踪系统 http://zipkin.io/，在微服务架构下，能够清晰的找出系统问题所在。它同时管理数据收集和数据查询。Zipkin的设计基于[Google Dagger](http://research.google.com/pubs/pub36356.html)论文
架构 查看您的平台是否已经在 instrumentation, 查看列表existing instrumentations
示例流程 以下是 User Code 调用资源/foo 的示例 http 跟踪序列，讲一个 span 在 User Code 收到 http 响应之后异步发送给 zipkin
 ┌─────────────┐ ┌───────────────────────┐ ┌─────────────┐ ┌──────────────────┐ │ User Code │ │ Trace Instrumentation │ │ Http Client │ │ Zipkin Collector │ └─────────────┘ └───────────────────────┘ └─────────────┘ └──────────────────┘ │ │ │ │ ┌─────────┐ │ ──┤GET /foo ├─▶ │ ────┐ │ │ └─────────┘ │ record tags │ │ ◀───┘ │ │ ────┐ │ │ │ add trace headers │ │ ◀───┘ │ │ ────┐ │ │ │ record timestamp │ │ ◀───┘ │ │ ┌─────────────────┐ │ │ ──┤GET /foo ├─▶ │ │ │X-B3-TraceId: aa │ ────┐ │ │ │X-B3-SpanId: 6b │ │ │ │ └─────────────────┘ │ invoke │ │ │ │ request │ │ │ │ │ │ │ ┌────────┐ ◀───┘ │ │ ◀─────┤200 OK ├─────── │ │ ────┐ └────────┘ │ │ │ record duration │ │ ┌────────┐ ◀───┘ │ ◀──┤200 OK ├── │ │ │ └────────┘ ┌────────────────────────────────┐ │ │ ──┤ asynchronously report span ├────▶ │ │ │ │{ │ │ &amp;quot;traceId&amp;quot;: &amp;quot;aa&amp;quot;, │ │ &amp;quot;id&amp;quot;: &amp;quot;6b&amp;quot;, │ │ &amp;quot;name&amp;quot;: &amp;quot;get&amp;quot;, │ │ &amp;quot;timestamp&amp;quot;: 1483945573944000,│ │ &amp;quot;duration&amp;quot;: 386000, │ │ &amp;quot;annotations&amp;quot;: [ │ │--snip-- │ └────────────────────────────────┘  Transport instrumentation(装配库)发送的 Span 必须由装配的服务传输到 Collector。有三种主要的传输类型：HTTP、Scribe 和 Kafka.</description>
    </item>
    
    <item>
      <title>解决 AWS ELB 偶发的 502 Bad Gateway 错误</title>
      <link>http://zhaox.xyz/posts/aws-elb-502/</link>
      <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/aws-elb-502/</guid>
      <description>问题描述 在使用了 Prometheus 做了 HTTP 协议的监控之后，blackbox_exporter 偶尔会报一些 ProbeDown 的报警，经过检查是 502 Bad Gateway 错误，但此时后端是正常的，只是在 AWS ELB 的监控指标中，看到了 ELB HTTP 5xx 相关错误，因此困扰了一段时间。
HTTP 数据流向如下：
 [Client] --- [ELB] --- [nginx] --- [App Servers]  排查问题 最开始是怀疑是后端问题，但是查阅了 nginx 和 App servers 的日志，没有任何结果，只是在 ELB 日志里面找到了 502 Bad Gateway 的错误信息。无奈之下甚至怀疑 nginx 所在 EC2 instance 有问题，因此求助了 AWS 技术支持。根据建议，在 nginx 这端做了 tcpdump 抓包，最后终于在 AWS 技术支持的帮助下，定位并解决了问题 🎉。
先补充一个知识：如果后端支持的话，ELB 会使用保持连接（HTTP persistent/keep-alive connections）。来看看这一个保持连接的 TCP stream：
其中，10.100.2.186 是 ELB 内部 IP，10.</description>
    </item>
    
    <item>
      <title>Gitlab omnibus 8.15.1 upgrade to 9.5.9</title>
      <link>http://zhaox.xyz/posts/gitlab-omnibus-8.15.1-upgrade-to-9.5.9/</link>
      <pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/gitlab-omnibus-8.15.1-upgrade-to-9.5.9/</guid>
      <description>升级场景 由于公司要通过 gitlab 接入 ci 和 cd 功能，经测试一个 repo 不能正常使用，且 gitlab9.5 之后增加了很多新功能，比较吸引我们
 GPG Commit Verification: GPG 密钥允许您验证签名提交
 New Navigation Improvements: 界面窗口有所改进，更便捷和美观，可以在老界面和新界面自由切换
 Project Template: 新增了更多的项目模板
 Automatic Retry for Failed CI Jobs：自动重试失败的 ci job
 Automatically Monitor Auto Deployed Apps：自动监控自动部署应用程序
 Merge Request Diff File Navigation：查看 merge request 时更清晰
  更多的特性请查阅：https://about.gitlab.com/2017/08/22/gitlab-9-5-released/
考虑的点及问题  1.postgresql 版本问题： 由于我们公司使用的是外部的 postgresql 和 redis，postgresql 的版本为 9.3，而 gitlab9.5.9 依赖 postgresql 9.6 以上的版本，所以在升级 gitlab 的时候他会升级数据库，但是我们是外部的(AWS RDS)，所以他并不能升级，会抛出异常。</description>
    </item>
    
    <item>
      <title>七个对我最重要的职业建议-译文</title>
      <link>http://zhaox.xyz/posts/2017-06-09-%E4%B8%83%E4%B8%AA%E5%AF%B9%E6%88%91%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E8%81%8C%E4%B8%9A%E5%BB%BA%E8%AE%AE-%E8%AF%91%E6%96%87/</link>
      <pubDate>Fri, 09 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-06-09-%E4%B8%83%E4%B8%AA%E5%AF%B9%E6%88%91%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E8%81%8C%E4%B8%9A%E5%BB%BA%E8%AE%AE-%E8%AF%91%E6%96%87/</guid>
      <description>七个对我最重要的职业建议（译文） 作者： 阮一峰
日期： 2015 年 9 月 18 日
Nicholas C. Zakas 是全世界最著名的 JavaScript 程序员之一。
两年前，他写了一篇长文，回顾自己的职业生涯，提到七个对他来说最重要的建议。
我读完很受启发，决定做一点摘录。你可以先读下面的精简版，再去读全文。
七个对我最好的职业建议（精简版） 作者：Nicholas C. Zakas
译者：阮一峰
原文网址：https://www.nczonline.net/blog/2013/10/15/the-best-career-advice-ive-received/
一、不要别人点什么，就做什么 我的第一份工作，只干了 8 个月，那家公司就倒闭了。我问经理，接下来我该怎么办，他说：
 &amp;ldquo;小伙子，千万不要当一个被人点菜的厨师，别人点什么，你就烧什么。不要接受那样一份工作，别人下命令你该干什么，以及怎么干。你要去一个地方，那里的人肯定你对产品的想法，相信你的能力，放手让你去做。&amp;rdquo;
 我从此明白，单单实现一个产品是不够的，你还必须参与决定怎么实现。好的工程师并不仅仅服从命令，而且还给出反馈，帮助产品的拥有者改进它。
二、推销自己 我进入雅虎公司以后，经理有一天跟我谈话，他觉得我还做得不够。
 &amp;ldquo;你工作得很好，代码看上去不错，很少出 Bug。但是，问题是别人都没看到这一点。为了让其他人相信你，你必须首先让别人知道你做了什么。你需要推销自己，引起别人的注意。&amp;rdquo;
 我这才意识到，即使做出了很好的工作，别人都不知道，也没用。做一个角落里静静编码的工程师，并不可取。你的主管会支持你，但是他没法替你宣传。公司的其他人需要明白你的价值，最好的办法就是告诉别人你做了什么。一封简单的 Email：&amp;rdquo;嗨，我完成了 XXX，欢迎将你的想法告诉我&amp;rdquo;，就很管用。
三、学会带领团队 工作几年后，已经没人怀疑我的技术能力了，大家知道我能写出高质量的可靠代码。有一次，我问主管，怎么才能得到提升，他说：
 &amp;ldquo;当你的技术能力过关以后，就要考验你与他人相处的能力了。&amp;rdquo;
 于是，我看到了，自己缺乏的是领导能力，如何带领一个团队，有效地与其他人协同工作，取到更大的成果。
四、生活才是最重要的 有一段时间，我在雅虎公司很有挫折感，对公司的一些做法不认同，经常会对别人发火。我问一个同事，他怎么能对这种事情保持平静，他回答：
 &amp;ldquo;你要想通，这一切并不重要。有人提交了烂代码，网站下线了，又怎么样？工作并不是你的整个生活。它们不是真正的问题，只是工作上的问题。真正重要的事情都发生在工作以外。我回到家，家里人正在等我，这才重要啊。&amp;rdquo;
 从此，我就把工作和生活分开了，只把它当作&amp;rdquo;工作问题&amp;rdquo;看待。这样一来，我对工作就总能心平气和，与人交流也更顺利了。
五、自己找到道路 我被提升为主管以后，不知道该怎么做。我请教了上级，他回答：
 &amp;ldquo;以前都是我们告诉你做什么，从现在开始，你必须自己回答这个问题了，我期待你来告诉我，什么事情需要做。&amp;rdquo;
 很多工程师都没有完成这个转变，如果能够做到，可能就说明你成熟了，学会了取舍。你不可能把时间花在所有事情上面，必须找到一个重点。
六、把自己当成主人 我每天要开很多会，有些会议我根本无话可说。我对一个朋友说，我不知道自己为什么要参加这个会，也没有什么可以贡献，他说：
 &amp;ldquo;不要再去开这样的会了。你参加一个会，那是因为你参与了某件事。如果不确定自己为什么要在场，就停下来问。如果这件事不需要你，就离开。不要从头到尾都静静地参加一个会，要把自己当成负责人，大家会相信你的。&amp;rdquo;
 从那时起，我从没有一声不发地参加会议。我确保只参加那些需要我参加的会议。
七、找到水平更高的人 最后，让我从自己的经历出发，给我的读者一个建议。
 &amp;ldquo;找到那些比你水平更高、更聪明的人，尽量和他们在一起，吃饭或者喝咖啡，向他们讨教，了解他们拥有的知识。你的职业，甚至你的生活，都会因此变得更好。&amp;rdquo;</description>
    </item>
    
    <item>
      <title>软技能——代码之外的生存指南</title>
      <link>http://zhaox.xyz/posts/%E8%BD%AF%E6%8A%80%E8%83%BD%E4%BB%A3%E7%A0%81%E4%B9%8B%E5%A4%96%E7%9A%84%E7%94%9F%E5%AD%98%E6%8C%87%E5%8D%97/</link>
      <pubDate>Fri, 09 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/%E8%BD%AF%E6%8A%80%E8%83%BD%E4%BB%A3%E7%A0%81%E4%B9%8B%E5%A4%96%E7%9A%84%E7%94%9F%E5%AD%98%E6%8C%87%E5%8D%97/</guid>
      <description>这本书是在逛知乎的时候发现的，说的神乎其神的，也没多想就买了本，并在一个周六的下午看完了，读的过程倒是很轻松，速度也挺快，因为毕竟是一本励志书，多少有点“鸡汤”。总体来说，这本书可圈可点，有干货，也有“鸡汤”，主要分为职业、自我营销、学习、生产力、理财、健身、精神七大模块，不论是不是码农，取其精华来安利一下自己还是不错的！ 下面我就谈谈书中一些不错的干货，湿货就自己去慢慢看吧。
一、如何对待上班这件事情？ 把自己当做一个软件企业，把雇主当做企业的一个客户，你应当能够提供某种产品或者服务（把一个想法通过技术手段变成一个产品的能力），不断提升你的服务质量，专注于为某一类客户提供特定的服务，做好自我营销，为更多更优质的雇主服务。
二、如何注意人际关系？ 不是教你搞办公室政治，而是让你在这上面少踩坑。书中有句话比较经典：“一旦你贬低他人，削弱他们的成就感，在某种程度上就如同切断了他们的氧气补给，获得的回馈将完全是抓狂和绝望”。
 所以切记不要贬低他人，而是应该多激励
 学会聚精会神地聆听，并指出问题所在以及相关解决方案
 在小事情上放弃立场或承认错误有时候能为你赢得意想不到的尊重
  三、如何搞定面试？ 作者的观点并不新颖，但是的确这种方式最有效，同时也说明了其他方式的不靠谱！
 找人内推
 即便不换工作也要多面试增加面试经验
  四、技术做到什么程度？ 是一个方向钻到底，还是什么都搞？一门技术钻的越深，潜在的机会就会越少，但获得这些工作机会的可能性就越大。所以我觉得规划好自己的技术栈很有必要，总体来说一专多能可能会好一些。永远不要陷入对技术的狂热之中，只要明白不同的场景需要不同的技术方案解决就行！
五、如何晋升？  承担更多的责任
 做了事情要及时反馈给上面，上面不知道一切都是徒劳
 提升自己的技能
 不是提出问题，而是解决问题，相信一切问题都可以解决
  六、如何创业？ 要利用业余时间做起来，后期到一定阶段再辞职也不迟，不仅降低了风险，还提高了成功率。
创业要从小处着手，也就是朝着某个独角兽方向发展，比如国内的 Face++，就是只做人脸识别算法。
七、技术人员如何自我营销？  写博客
 社交媒体
 演讲、培训别人
 写书
  八、如何学习？  培养自学能力
 筛选出重点，快速突破
 动手实践才是王道
  九、如何管好自己？ 中国的教育模式导致我们基本上都是靠外部因素来左右我们的行为，很少有自我驱动型。良好的生活习惯是自律的有效保证，所以从现在开始让自己的生活变得井然有序，培养起自己的生活习惯！改掉坏习惯，培养好习惯，把大的目标转换为一个个小的计划！
十、时间去哪了？  看手机推送的所谓新闻（实际上都是毫无营养的标题党）
 看视频
 沉迷于刷社交软件
  十一、为何你总是逃避努力工作？ 努力工作——&amp;gt;辛苦——&amp;gt;有价值的东西——&amp;gt;带来的幸福感持久</description>
    </item>
    
    <item>
      <title>Nginx配置文件安全分析工具Gixy</title>
      <link>http://zhaox.xyz/posts/2017-06-01-nginx%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%AE%89%E5%85%A8%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7gixy/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-06-01-nginx%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%AE%89%E5%85%A8%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7gixy/</guid>
      <description>Nginx 配置文件安全分析工具 Gixy Gixy 是一个分析 Nginx 配置的工具。Gixy 的主要目的是防止安全的错误配置和自动化探伤。
目前支持 Python 版本 2.7 和 3.5 +。
免责声明:Gixy 只在 GNU / Linux Gixy 测试良好,其他的操作系统可能有问题。
Gixy 的特性  找出服务器端请求伪造。 验证 HTTP 拆分。 验证 referrer/origin 问题。 验证是否正确通过 add_header 指令重新定义 Response Headers。 验证请求的主机头是否伪造。 验证 valid_referers 是否为空。 验证是否存在多行主机头。  Github 地址：https://github.com/yandex/gixy
Gixy 安装 安装步骤很简单，直接使用 pip 安装即可
$ sudo pip install gixy # 注意：目前支持Python版本2.7和3.5 +。  Gixy 使用 Gixy 默认检查/etc/nginx/nginx.conf 文件
$ gixy  可以指定 nginx 配置文件所在位置</description>
    </item>
    
    <item>
      <title>基于amazon-s3创建yum仓库</title>
      <link>http://zhaox.xyz/posts/2017-03-17-%E5%9F%BA%E4%BA%8Eamazon-s3%E5%88%9B%E5%BB%BAyum%E4%BB%93%E5%BA%93/</link>
      <pubDate>Fri, 17 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-03-17-%E5%9F%BA%E4%BA%8Eamazon-s3%E5%88%9B%E5%BB%BAyum%E4%BB%93%E5%BA%93/</guid>
      <description>AWS 配置 1.创建 s3 仓库  aws s3 mb s3://yum-repo-s3 make_bucket: yum-repo-s3  2.创建 s3 用户  $~ aws iam create-user --user-name yum-repo-user  3.赋予 s3 用户权限  $ aws iam put-user-policy --user-name yum-repo-user --policy-name yum-repo-s3-Bucket-Access --policy-document file://demo-s3-rpm-repo_policy.json $ cat demo-s3-rpm-repo_policy.json { &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;, &amp;quot;Statement&amp;quot;: [ { &amp;quot;Sid&amp;quot;: &amp;quot;Stmt1489722431765&amp;quot;, &amp;quot;Action&amp;quot;: &amp;quot;s3:*&amp;quot;, &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;, &amp;quot;Resource&amp;quot;: &amp;quot;arn:aws-cn:s3:::yum-repo-s3&amp;quot; } ] }  4.查看其权限  $ aws iam list-user-policies --user-name yum-repo-user { &amp;quot;PolicyNames&amp;quot;: [ &amp;quot;yum-repo-s3-Bucket-Access&amp;quot; ] }  5.</description>
    </item>
    
    <item>
      <title>docker 1.13&#43; FORWARD链为DROP的问题</title>
      <link>http://zhaox.xyz/posts/2017-03-16-docker1.13&#43;%E7%89%88%E6%9C%ACforward%E9%93%BE%E4%B8%BAdrop%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-03-16-docker1.13&#43;%E7%89%88%E6%9C%ACforward%E9%93%BE%E4%B8%BAdrop%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>本地网络容器访问漏洞 这个问题出现在 docker 的issue中
其主要影响：允许与 docker 主机位于同一网络上的任何人访问在该主机上运行的容器，而不是只访问暴露的端口。
当 docker 启动时，它启用 net.ipv4.ip_forward 而不改变 iptables FORWARD 链默认策略 DROP。这意味着与 docker 主机位于同一网络上的另一台机器可以向其路由表添加路由，并直接寻址在该 docker 主机上运行的任何容器。
例如，如果 docker0 子网是 172.17.0.0/16（默认子网），并且 docker 主机的 IP 地址是 192.168.0.10，则从网络上的另一个主机运行：
 ip route add 172.17.0.0/16 via 192.168.0.10 nmap 172.17.0.0/16  上面将扫描主机上运行的容器，并报告找到的 IP 地址和运行的服务。
要解决这个问题，docker 需要将 FORWARD 策略设置为 DROP 启用 net.ipv4.ip_forwardsysctl 参数。
带来的其他问题 带来的问题主要出现在 docker 配合 kubernetes 使用时，kubernetes 的网络使用 flannel，flannel 可以使不同主机间的容器通信。如下图
但是带来的问题是：一旦 DROP 后，不同主机间就不能正常通信了，为了保证其能正常通信，可以选择三种方案。
1.将 FORWARD 改为 ACCEPT  $ iptables -P FORWARD ACCEPT 这种方式不适合重启，一重启就需要修改  2.</description>
    </item>
    
    <item>
      <title>Docker 社区和企业版出现了</title>
      <link>http://zhaox.xyz/posts/2017-03-15-docker-%E7%A4%BE%E5%8C%BA%E5%92%8C%E4%BC%81%E4%B8%9A%E7%89%88%E5%87%BA%E7%8E%B0%E4%BA%86/</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-03-15-docker-%E7%A4%BE%E5%8C%BA%E5%92%8C%E4%BC%81%E4%B8%9A%E7%89%88%E5%87%BA%E7%8E%B0%E4%BA%86/</guid>
      <description>Docker 社区和企业版出现了 今天早上去官网看文档，一进去发现主页变了，映入眼帘的便是：join us at Dockercon, April 17,2017。一脸蒙蔽的我往下翻了翻，发现做了这么久的解决方案，docker 企业版都支持，但是收费！！！！而且，Dockercon 是什么鬼？
当然，docker 也被分为两个版本：
 community-edition
 enterprise-edition
  dockercon 社区 docker 官网公布了一个社区，目前还没有开放，其域名是:2017.dockercon.com
docker 社区将在 2017 年 4 月 17 号开始使用，现在可以去注册了。
Docker CE 和 EE 的区别及定价 参照：https://www.docker.com/pricing
Docker EE（企业版）详细介绍  Docker 企业版（EE）专为企业开发和 IT 团队设计，可在大规模生产中构建，运送和运行关键业务应用程序。Docker EE 集成，认证和支持，为企业提供业界最安全的容器平台，实现所有应用程序的现代化。作为一个以应用为中心的平台，Docker EE 旨在加速和保护整个软件供应链，从开发到在任何基础设施上运行的生产。
 平台 信任和认证  Docker EE 为在企业 Linux 或 Windows 操作系统和云提供商上运行的应用程序提供集成，测试和认证的平台。Docker EE 紧密集成到底层基础架构，以提供本机，易于安装的体验和优化的 Docker 环境。Docker 认证的基础架构，容器和插件专用于 Docker EE，由 Docker 和认证技术合作伙伴提供合作支持。
  认证基础设施为企业 Linux（CentOS，Oracle Linux，RHEL，SLES，Ubuntu）提供集成环境 Windows Server 2016 和云提供商如 AWS 和 Azure</description>
    </item>
    
    <item>
      <title>COW (Climb Over the Wall) Proxy</title>
      <link>http://zhaox.xyz/posts/2017-02-20-cow-climb-over-the-wall-proxy/</link>
      <pubDate>Mon, 20 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-02-20-cow-climb-over-the-wall-proxy/</guid>
      <description>COW 简介  COW 是一个简化穿墙的 HTTP 代理服务器。它能自动检测被墙网站，仅对这些网站使用二级代理。COW 对外提供一个地址和端口，用户只需将这个地址配置好，便可以使用。COW 的二级代理用来访问海外的 server，支持sock5、shadowsocks、cow(采用 shadowsock 协议)三种方式. Github 地址
 功能 COW 的设计目标是自动化，理想情况下用户无需关心哪些网站无法访问，可直连网站也不会因为使用二级代理而降低访问速度。
 作为 HTTP 代理，可提供给移动设备使用；若部署在国内服务器上，可作为 APN 代理
 支持 HTTP, SOCKS5, shadowsocks 和 cow 自身作为二级代理
 可使用多个二级代理，支持简单的负载均衡
 自动检测网站是否被墙，仅对被墙网站使用二级代理
 自动生成包含直连网站的 PAC，访问这些网站时可绕过 COW
 内置常见可直连网站，如国内社交、视频、银行、电商等网站（可手工添加）
  一、安装 1. OS X, Linux (x86, ARM): 执行以下命令（也可用于更新） curl -L git.io/cow | bash # 环境变量 `COW_INSTALLDIR` 可以指定安装的路径，若该环境变量不是目录则询问用户  2.Windows: 从 release 页面下载 3.熟悉 Go 的用户可用 go get github.</description>
    </item>
    
    <item>
      <title>Cow-rc-样例配置文件</title>
      <link>http://zhaox.xyz/posts/2017-02-20-cow-rc-%E6%A0%B7%E4%BE%8B%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/</link>
      <pubDate>Mon, 20 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-02-20-cow-rc-%E6%A0%B7%E4%BE%8B%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/</guid>
      <description>// 配置文件中 # 开头的行为注释 // // 代理服务器监听地址，重复多次来指定多个监听地址，语法： // // listen = protocol://[optional@]server_address:server_port // // 支持的 protocol 如下： // // HTTP (提供 http 代理): // listen = http://127.0.0.1:7777 // // 上面的例子中，cow 生成的 PAC url 为 http://127.0.0.1:7777/pac // 配置浏览器或系统 HTTP 和 HTTPS 代理时请填入该地址 // 若配置代理时有对所有协议使用该代理的选项，且你不清楚此选项的含义，请勾选 // // cow (需两个 cow 服务器配合使用): // listen = cow://encrypt_method:password@1.2.3.4:5678 // // 若 1.2.3.4:5678 在国外，位于国内的 cow 配置其为二级代理后，两个 cow 之间可以 // 通过加密连接传输 http 代理流量。目前的加密采用与 shadowsocks 相同的方式。 // // 其他说明： // - 若 server_address 为 0.</description>
    </item>
    
    <item>
      <title>AWS EBS 在线扩容</title>
      <link>http://zhaox.xyz/posts/2017-02-17-aws-%E5%9C%A8%E7%BA%BF%E6%89%A9%E5%AE%B9/</link>
      <pubDate>Fri, 17 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-02-17-aws-%E5%9C%A8%E7%BA%BF%E6%89%A9%E5%AE%B9/</guid>
      <description>AWS Management Console 扩容 ebs 容量 原文连接：https://aws.amazon.com/blogs/aws/amazon-ebs-update-new-elastic-volumes-change-everything/
1.选择扩容设备 2.修改扩容容量 4.等待其状态达到 100% 扩容磁盘分区 growpart
 growpart /dev/xvdf 1  parted
 GNU Parted 2.3 Using /dev/xvdf1 Welcome to GNU Parted! Type &#39;help&#39; to view a list of commands. (parted) print Model: Unknown (unknown) Disk /dev/xvdf1: 80.7GB Sector size (logical/physical): 512B/512B Partition Table: loop Number Start End Size File system Flags 1 0.00B 53.7GB 53.7GB ext4 (parted) resizepart 1 80.7GB # resizepart NUMBER END Warning: Partition /dev/xvdf1 is being used.</description>
    </item>
    
    <item>
      <title>Yahoo-screwdriver开源</title>
      <link>http://zhaox.xyz/posts/2017-02-08-yahoo-screwdriver-%E5%BC%80%E6%BA%90%E4%BA%86/</link>
      <pubDate>Wed, 08 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-02-08-yahoo-screwdriver-%E5%BC%80%E6%BA%90%E4%BA%86/</guid>
      <description>screwdriver 介绍 screwdriver 是 yahoo 开源的一款发布工具，用于大规模持续交付到生产的动态基础架构.
这应该是目前最完整拥有 CI(持续集成 Continuous integration)和 CD(持续交付 Continuous delivery)的专案.
 官网: http://screwdriver.cd/ Doc: http://docs.screwdriver.cd/user-guide/quickstart/ Github: https://github.com/screwdriver-cd Docker: https://hub.docker.com/u/screwdrivercd/  特色  1.使部署通道更容易 (Making deployment pipelines easy)
 2.优化主干开发(Optimizing for trunk development)
 3.回滚更加容易(Making rolling back easy)
  详情请阅:yahooeng.tumblr.com
Screwdrive 架构和开发流程 详情请查看：http://docs.screwdriver.cd/cluster-management/
组件:  1.REST API(与流水线协同工作的接口)
 2.Web UI(用于流水线 API 的可视化接口)
 3.Launcher（启动器）: 设置环境并执行 shell 命令的工具
 4.Execution Engine（执行引擎）：可插拔的构建执行器，支持在容器（Jenkins、Kubernetes、Mesos、Docker Swarm）内执行命令。
 5.Datastore（数据存储）：可插拔的 NoSQL 存储，用于维护流水线配置数据（DynamoDB、MongoDB、CouchDB、Postgres）。执行引擎和数据存储都使用了可插拔的架构，使得用户可按自身意向选用引擎。
   作者：Antony WX&amp;amp;QQ：1257465991 Q/A：如有问题请慷慨提出</description>
    </item>
    
    <item>
      <title>Gitlab-Omnibus</title>
      <link>http://zhaox.xyz/posts/2017-01-23-gitlab-omnibus/</link>
      <pubDate>Sat, 04 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-01-23-gitlab-omnibus/</guid>
      <description>一、Gitlab 简介 组件说明
 前端：Nginx，用于页面及 Git tool 走 http 或 https 协议
 后端：Gitlab 服务，采用 Ruby on Rails 框架，通过 unicorn 实现后台服务及多进程
 SSHD：开启 sshd 服务，用于用户上传 ssh key 进行版本克隆及上传。注：用户上传的 ssh key 是保存到 git 账户中
 数据库：目前仅支持 MySQL 和 PostgreSQL
 Redis：用于存储用户 session 和任务，任务包括新建仓库、发送邮件等等
 Sidekiq：Rails 框架自带的，订阅 redis 中的任务并执行
  版本说明
 CE(GitLab Community Edition)：社区版(源码安装方式) https://docs.gitlab.com/ce/README.html
 OM(Omnibus GitLab)：综合版(包安装方式) https://docs.gitlab.com/omnibus/README.html
 EE(GitLab Enterprise Edition)：企业版 https://docs.gitlab.com/ee/README.html
  二、Gitlab Omnibus 安装和配置 1.</description>
    </item>
    
    <item>
      <title>Ruby 环境那些坑</title>
      <link>http://zhaox.xyz/posts/2017-01-23-ruby%E7%8E%AF%E5%A2%83%E9%82%A3%E4%BA%9B%E5%9D%91/</link>
      <pubDate>Mon, 23 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-01-23-ruby%E7%8E%AF%E5%A2%83%E9%82%A3%E4%BA%9B%E5%9D%91/</guid>
      <description>最近接触了很多 ruby 程序，除了 java 之外，让我感觉第二耗时、不是很舒服的语言。但是 ruby 不让人舒服主要还是网络方面的问题，不过只要稍微配置就能让装 ruby 程序变得飞快，不用一安装一天。本文做个总结和梳理
先介绍下 Ruby 相关的各种概念（rvm, gem, bundle, rake, rails 等） Ruby 这个就不用多说了
RVM 用于帮你安装 Ruby 环境，帮你管理多个 Ruby 环境，帮你管理你开发的每个 Ruby 应用使用机器上哪个 Ruby 环境。Ruby 环境不仅仅是 Ruby 本身，还包括依赖的第三方 Ruby 插件。都由 RVM 管理。
Rails 这个也不用多说，著名开发框架。详细看 http://zh.wikipedia.org/wiki/Ruby_on_Rails
RubyGems RubyGems 是一个方便而强大的 Ruby 程序包管理器（ package manager），类似 RedHat 的 RPM.它将一个 Ruby 应用程序打包到一个 gem 里，作为一个安装单元。无需安装，最新的 Ruby 版本已经包含 RubyGems 了。
Gem Gem 是封装起来的 Ruby 应用程序或代码库。
注：在终端使用的 gem 命令，是指通过 RubyGems 管理 Gem 包。</description>
    </item>
    
    <item>
      <title>添加一个用户sudo权限的使用密钥登录的用户</title>
      <link>http://zhaox.xyz/posts/2017-01-22-useradd-sh/</link>
      <pubDate>Sat, 21 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-01-22-useradd-sh/</guid>
      <description> $1为用户名 $2为用户公钥 #!/bin/bash #添加证书账户,并且将对应用户加入sudo # $1 为用户名 $2 为用户公钥 [[ -z $3 ]] &amp;amp;&amp;amp; useradd $1 || useradd -d $3 $1 mkdir /home/$1/.ssh echo &amp;quot;$2&amp;quot; &amp;gt;/home/$1/.ssh/authorized_keys chown -R $1.$1 /home/$1/.ssh/ chmod 600 /home/$1/.ssh/authorized_keys chmod 700 /home/$1/.ssh/ sed -i &#39;/^root.*/a\&#39;$1&#39; ALL=(ALL) NOPASSWD:ALL&#39; /etc/sudoers  </description>
    </item>
    
    <item>
      <title>Linux系统异常-CoreDump详解</title>
      <link>http://zhaox.xyz/posts/2017-01-19-coredump/</link>
      <pubDate>Thu, 19 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-01-19-coredump/</guid>
      <description>CoreDump http://linux.die.net/man/5/core
什么是 core dump core dump 又叫核心转储, 当程序运行过程中发生异常, 程序异常退出时, 由操作系 统把程序当前的内存状况存储在一个 core 文件中, 叫 core dump。core dump 在应用 crash 掉之后对问题的诊断是很有帮助的。而在默认安装的时候 core dump 是关闭状 态的。
如何查看系统是否打开了 core dump  Note：使用 ulimit -c 查看 core dump 是否打开。如果结果为 0，则表示此功能处于关闭 状态，不会生成 core 文件
  $ ulimit -c  如何打开 core dump  方法一:命令行方式
 $ ulimit -c 1024 # 避免一下生成几G的大文件 $ ulimit -c unlimited #无限制  方法二:配置 profile 文件: 将ulimit -S -c 0 &amp;gt; /dev/null 2&amp;gt;&amp;amp;1改成ulimit -S -c unlimited &amp;gt; /dev/null 2&amp;gt;&amp;amp;1</description>
    </item>
    
    <item>
      <title>IPSEC-L2TP-VPN-on-Ubuntu-14.04-with-OpenSwan-xl2tpd-and-ppp</title>
      <link>http://zhaox.xyz/posts/2017-01-13-ipsec-l2tp-vpn-on-ubuntu-14.04-with-openswan-xl2tpd-and-ppp/</link>
      <pubDate>Fri, 13 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-01-13-ipsec-l2tp-vpn-on-ubuntu-14.04-with-openswan-xl2tpd-and-ppp/</guid>
      <description>VPN 原理 http://www.cisco.com/support/zh/105/IPSECpart1.shtml#glossary
http://www.china-ccie.com/doc/vpn/vpn.html
Install ppp openswan and xl2tpd  $ sudo apt-get install openswan xl2tpd ppp lsof  Firewall and sysctl 配置一条防火墙语句  iptables -t nat -A POSTROUTING -s %SERVERIP% -o eth0 -j MASQUERADE  启用内核转发和禁用 ICP redirects  echo &amp;quot;net.ipv4.ip_forward = 1&amp;quot; | tee -a /etc/sysctl.conf echo &amp;quot;net.ipv4.conf.all.accept_redirects = 0&amp;quot; | tee -a /etc/sysctl.conf echo &amp;quot;net.ipv4.conf.all.send_redirects = 0&amp;quot; | tee -a /etc/sysctl.conf echo &amp;quot;net.ipv4.conf.default.rp_filter = 0&amp;quot; | tee -a /etc/sysctl.</description>
    </item>
    
    <item>
      <title>aws-ec2-双网卡问题y</title>
      <link>http://zhaox.xyz/posts/2017-01-12-aws-ec2-%E5%8F%8C%E7%BD%91%E5%8D%A1%E9%97%AE%E9%A2%98/</link>
      <pubDate>Thu, 12 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-01-12-aws-ec2-%E5%8F%8C%E7%BD%91%E5%8D%A1%E9%97%AE%E9%A2%98/</guid>
      <description>问题描述 在已存在的 EC2 上新添加网卡后发现，凡是和 eth0 在同一个网段的只能通过 eth0 访问，不能通过 eth1。同样的，在 eth1 网段的只能通过 eth1 访问，不能通过 eth0 访问。 如果既不在 eth0 也不再 eth0 默认走 eth0(在没有修改路由表的前提下，默认路由是 eth0)
问题分析 之所以出现您列出的网络访问现象，是因为目前的实例当中有两块网卡，而发生故障的时候，路由的走向是从网卡 2 进来的数据包从网卡 1 发送出去，或者从网卡 1 进来的数据包从网卡 2 发送出去，AWS 底层会把这样的数据包丢弃。
因此需要手动定义策略路由，在响应网卡 1 进来的数据包时通过网卡 1 发送，响应网卡 2 进来的数据包时通过网卡 2 发送。
解决方案 关于这个现象和解决方案，可以参考这遍文档：
http://www.linuxjournal.com/article/7291
该文档较长，这里介绍一个配置路由策略的事例，可以按照此事例的方法结合具体网络环境进行配置：
1、首先为网卡 1 和 2 创建各自的路由表：  ip route add [子网1网段] via [您子网的网关IP] dev eth0 tab 1 ip route add [子网2网段] via [您子网的网关IP] dev eth1 tab 2 可以通过ip route show table 1和ip route show table 2查看您刚刚完成的配置是否正确  2、然后创建策略路由  ip rule add from [eth0的IP]/32 tab 1 priority 500 ip rule add from [eth1的IP]/32 tab 2 priority 600  这个配置的意思是，将原地址为 eth0 的 IP 的包按照路由表 1 发送,将原地址为 eth1 的 IP 的包按照路由表 2 发送</description>
    </item>
    
    <item>
      <title>aws自定义AMI-UUID相同处理方法</title>
      <link>http://zhaox.xyz/posts/2017-01-12-aws%E8%87%AA%E5%AE%9A%E4%B9%89ami-uuid%E7%9B%B8%E5%90%8C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/</link>
      <pubDate>Thu, 12 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-01-12-aws%E8%87%AA%E5%AE%9A%E4%B9%89ami-uuid%E7%9B%B8%E5%90%8C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/</guid>
      <description>Note: 在制作 Centos 7 AMI 并进行使用时，我们发现：如果将两个相同 AMI 挂载到一个操作系统的时候，会出现因为 UUID 相同，导致不能挂载的问题(至于为什么将两个相同的 AMI 挂载到一个操作系统：当误删除用户的密钥文件或者不能登录系统时，我们不得不将其挂载到其他操作系统)
 问题重现  $ sudo fdisk -l Disk /dev/xvda: 53.7 GB, 53687091200 bytes, 104857600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x0001783d Device Boot Start End Blocks Id System /dev/xvda1 * 2048 104856254 52427103+ 83 Linux Disk /dev/mapper/docker-202:1-84149041-pool: 107.</description>
    </item>
    
    <item>
      <title>FastDFS unti for systemd</title>
      <link>http://zhaox.xyz/posts/fastdfs-unti-for-systemd/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/fastdfs-unti-for-systemd/</guid>
      <description>FastDFS tracker 的 unit 脚本 # Systemd unit file for default fastdfs_tracker # [Unit] Description=FastDFS tracker script After=syslog.target network.target [Service] Type=notify ExecStart=/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf [Install] WantedBy=multi-user.target  FastDFS storage 的 unit 脚本 # Systemd unit file for default fastdfs storage # [Unit] Description=FastDFS storage script After=syslog.target network.target [Service] Type=notify ExecStart=/usr/bin/fdfs_storaged /etc/fdfs/storage.conf [Install] WantedBy=multi-user.target  使用方法 复制到/usr/lib/systemd/system/下，建议命名为: fdfs_storaged.service和fdfs_tracker.service</description>
    </item>
    
    <item>
      <title>Git基础</title>
      <link>http://zhaox.xyz/posts/git/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/git/</guid>
      <description>一、git install  Ubuntu sudo apt-get install git Centos yum install git Mac brew install git  二、git initialization configure  git config --global user.name &amp;quot;Your Name&amp;quot; git config --global user.email &amp;quot;email@example.com&amp;quot;  注意git config命令的--global参数，用了这个参数，表示你这台机器上所有的 Git 仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和 Email 地址 三、git create repository  git 版本库，也叫仓库，英文为repository，可以理解为一个目录，目录里的文件都可以被 git 管理起来，每个文件的修改、删除 Git 都能跟踪，以便任何时候都能够追踪历史，或者将来在某个时刻可以还原。  1、创建一个目录  $ mkdir gitrepo $ cd gitrepo $ pwd /Users/antony/gitrepo  注意：目录不要包含中文
2、通过 git init 命令把这个目录变成 Git 可以管理的仓库  $ git init Initialized empty Git repository in /Users/antony/gitrepo/.</description>
    </item>
    
    <item>
      <title>Golang 数组和切片</title>
      <link>http://zhaox.xyz/posts/2017-12-14-golang-%E6%95%B0%E7%BB%84%E5%92%8C%E5%88%87%E7%89%87/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-12-14-golang-%E6%95%B0%E7%BB%84%E5%92%8C%E5%88%87%E7%89%87/</guid>
      <description>一、数组类型 数组是 Go 语言编程中最常用的数据结构之一。顾名思义，数组就是指一系列同一类型数据 的集合。数组中包含的每个数据被称为数组元素(element)，一个数组包含的元素个数被称为数 组的长度。
1.定义 var 变量名 [len]type
 var a[5] int var a[5]string var a[15]bool  2.元素访问 可以使用数组下标来访问数组中的元素。与 C 语言相同，数组下标从 0 开始，len(array)-1 则表示最后一个元素的下标。下面的示例遍历整型数组并逐个打印元素内容:
 var a[5]int var b = [2]int {1,2} fmt.Printf(&amp;quot;%v %v %v\n&amp;quot;,a, b, b[1]) Output: [0 0 0 0 0] [1 2] 2  3.值类型 需要特别注意的是，在 Go 语言中数组是一个值类型( value type)。所有的值类型变量在赋值和作为参数传递时都将产生一次复制动作。如果将数组做为函数的从参数类型，则在函数调用时该参数将发生数据复制。因此，在函数体中无法修改传入的数组的内容，因为函数内操作的只是所传入数组的一个副本。
 $ vim main.go package main import ( &amp;quot;fmt&amp;quot; ) func modify(array [5]int) { array[0] = 100 fmt.</description>
    </item>
    
    <item>
      <title>Linux grep</title>
      <link>http://zhaox.xyz/posts/linux%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bgrep/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/linux%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bgrep/</guid>
      <description>一、Linux 文本处理三剑客  Linux 上有三种常用的文本处理工具，分别为：grep（egrep、fgrep）、sed、awk。今天主要给大家介绍一下三剑客中的第一剑：grep 伐木累。
 二、grep 是什么？  grep 全称（Globally search a Regular Expression and Print）是一个文本搜索工具，基于“pattern”（这里指的是过滤模式，多指正则表达式）对给定的文本进行搜索。
grep 家族：
  1.grep：支持使用基本正则表达式； 2.egrep：支持使用扩展正则表达式； 3.fgrep：不支持使用正则表达式； # 虽然正则表达式有强大的引擎，但是在单独过滤字符上面 fgrep要起到很大作用，这个作用在日志文件小的时候可能体现不出来，但是当文件达到几亿行就能体现出fgrep的搜索效率。（对大型web网站一天的日志量到几亿行是很轻松的，甚至更多）  三、grep 与 egrep 的主要参数 在介绍正则表达式前，先介绍一下 grep 的常用参数，这里所有涉及正则表达式 meta 字符都会在后续中展开！ 常用选项： &amp;ndash;color=auto：对匹配到的文本着色后高亮显示；（默认 centos7 对 grep、egrep、fgrep 已经设置参数，此处不做过多例子）
alias alias cp=&#39;cp -i&#39; alias egrep=&#39;egrep --color=auto&#39; alias fgrep=&#39;fgrep --color=auto&#39; alias grep=&#39;grep --color=auto&#39; alias l.=&#39;ls -d .* --color=auto&#39; alias ll=&#39;ls -l --color=auto&#39; alias ls=&#39;ls --color=auto&#39; alias mv=&#39;mv -i&#39; alias rm=&#39;rm -i&#39; alias which=&#39;alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde&#39;  -i：忽略字符大小写； # cat test Hello World How are you?</description>
    </item>
    
    <item>
      <title>Linux用户、用户组及权限管理</title>
      <link>http://zhaox.xyz/posts/2017-01-07-linux%E7%94%A8%E6%88%B7%E7%94%A8%E6%88%B7%E7%BB%84%E5%8F%8A%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-01-07-linux%E7%94%A8%E6%88%B7%E7%94%A8%E6%88%B7%E7%BB%84%E5%8F%8A%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/</guid>
      <description>一、Linux 用户及用户组的基本概念 用户：用户是实现能够将有限的资源在多个使用者之间进行分配；、 用户组：用户组是指多个用户的集合，方便对一类需要同样权限的用户授权 Linux 是多用户、多任务的操作系统。 多用户指：多人同时使用系统资源；多任务：同时运行多个进程
二、用户及用户组类别 1、用户：名称解析库 /etc/passwd a、管理员 root 用户标识（UID）为 0 b、普通用户及系统用户 普通用户的用户标识（既 UID）： CentOS 5,6: 500+ CentOS 7: 1000+ 系统用户用户标识（既 UID）： CentOS 5,6: 1-499 CentOS 7: 1-999
2、用户组：名称解析库 /etc/group a、管理员组 组标识为：0 b、普通用户组及系统用户组 普通用户组标识： CentOS 5,6: 500+ CentOS 7: 1000+ 系统用户组标识： CentOS 5,6: 1-499 CentOS 7: 1-999 3、用户组类别： 以用户为核心分为： 用户的主组：基本组； 用户的附加组：额外组； 以容纳的用户来划分： 私有组：与用户名相同，且只有一个用户； 共有组：组内包含了多个用户；
三、用户及用户组的认证机制 Linux 的用户密码认证方式在 centos7 中使用 sha512 认证信息库存储位置： 用户的认证信息库：/etc/shadow 组的认证信息库：/etc/gshadow 密码：加密存放，使用单向加密机制 算法： md5: message digest, 128bits sha1: secure hash algorithm, 160bits sha224 sha256 sha384 sha512</description>
    </item>
    
    <item>
      <title>Nginx&#43;PHP编译安装</title>
      <link>http://zhaox.xyz/posts/2017-01-07-nginx-php%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-01-07-nginx-php%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/</guid>
      <description>编译安装 Nginx&amp;amp;PHP nginx 编译安装 下载路径：(建议使用Tengine) http://nginx.org/ http://tengine.taobao.org/
启动命令 /usr/local/nginx/sbin/nginx 重载 nginx /usr/local/nginx/sbin/nginx -s reload
一、安装环境 sudo yum groupinstall &amp;quot;Development Tools&amp;quot; -y sudo yum install jemalloc-devel openssl-devel pcre-devel -y nginx编译参数(红色部分可能不支持,可去掉) # sudo ./configure --prefix=/usr/local/nginx --user=webapps --group=webapps --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-pcre --with-jemalloc # sudo make # sudo make install # cd /usr/local/nginx # sudo mv logs /export/ # sudo ln -s /export/logs logs # cd /usr/local/nginx/conf # sudo mkdir vhosts  二、修改 conf 文件 1.</description>
    </item>
    
    <item>
      <title>ansible脚本</title>
      <link>http://zhaox.xyz/posts/ansible%E8%84%9A%E6%9C%AC/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/ansible%E8%84%9A%E6%9C%AC/</guid>
      <description>这是一个可以自动在多台 server 批量安装及拷贝配置文件启动服务器的 ansible 脚本，本人学疏才浅，目前还有很多待完善，希望各位多多指教。
 #!/bin/bash # Author: Altamob # Directory tree # /opt/ # ├── ansible.sh	# like this,nginx.conf is varia $FILE # └── nginx.conf # Host IP One HOST=172.18.4.62 # Host IP two HOST1= # conf file template FILE=nginx.conf cat &amp;lt;&amp;lt; EOF +---------------------------------------+ | this is a ansible script | | from Altamob....... | +--------------------------------------- EOF read -p &amp;quot;pelease input your service name: &amp;quot; SER read -p &amp;quot;pelease input your hostgroup name: &amp;quot; HOSTGROUP mkdir /etc/ansible/roles/${SER}/{tasks,templates,vars,handlers} -pv &amp;amp;&amp;gt;/dev/null cp $FILE /etc/ansible/roles/${SER}/templates/${FILE}.</description>
    </item>
    
    <item>
      <title>python-快速改造：基础知识</title>
      <link>http://zhaox.xyz/posts/2017-01-07-python-%E5%BF%AB%E9%80%9F%E6%94%B9%E9%80%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/2017-01-07-python-%E5%BF%AB%E9%80%9F%E6%94%B9%E9%80%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</guid>
      <description>一、python 安装之 pyenv windows 下安装 Python （1）打开 web 浏览器，访问 Python 官方站点http://www.python.org
（2）点击 Download，或者在首页点击下载
（3）下载所需版本，目前最新版本为 3.5.2
（4）下载完成后双击并执行安装
CentOS 下安装 Python 和 pyenv  pyenv：pyenv 是一款 python 版本管理器，由于不同程序员可能使用不同的 python 进行开发，但还需在同一台机器又互不影响。pyenv 主要的作用就是可以设置不同的目录使用不同的版本，且可以很简单的安装 python。
 1、安装依赖包 yum groupinstall &amp;quot;Development Tools&amp;quot; &amp;quot;Server Plataform Development&amp;quot; -y  2、安装 pyenv （1）通过 git 克隆 pyenv 程序
$ git clone https://github.com/yyuu/pyenv.git ~/.pyenv  （2）设置 pyenv 的环境变量
$ echo &#39;export PYENV_ROOT=&amp;quot;$HOME/.pyenv&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile $ echo &#39;export PATH=&amp;quot;$PYENV_ROOT/bin:$PATH&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile  （3）添加pyenv init到环境变量中</description>
    </item>
    
    <item>
      <title>为什么你的Linux物理内存还有很多，却开始使用swap?</title>
      <link>http://zhaox.xyz/posts/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E7%9A%84linux%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E8%BF%98%E6%9C%89%E5%BE%88%E5%A4%9A%E5%8D%B4%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8swap/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E7%9A%84linux%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E8%BF%98%E6%9C%89%E5%BE%88%E5%A4%9A%E5%8D%B4%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8swap/</guid>
      <description>#Swap
现在的机器上都是有多个CPU和多个内存块的。以前我们都是将内存块看成是一大块内存，所有CPU到这个共享内存的访问消息是一样的。
这就是之前普遍使用的SMP模型。但是随着处理器的增加，共享内存可能会导致内存访问冲突越来越厉害，且如果内存访问达到瓶颈的时候，性能就不能随之增加。NUMA（Non-Uniform Memory Access）就是这样的环境下引入的一个模型。比如一台机器是有 2 个处理器，有 4 个内存块。我们将 1 个处理器和两个内存块合起来，称为一个NUMA node，这样这个机器就会有两个NUMA node。在物理分布上，NUMA node的处理器和内存块的物理距离更小，因此访问也更快。比如这台机器会分左右两个处理器（cpu1, cpu2），在每个处理器两边放两个内存块(memory1.1, memory1.2, memory2.1,memory2.2)，这样NUMA node1的 cpu1 访问memory1.1和memory1.2就比访问memory2.1和memory2.2更快。所以使用NUMA的模式如果能尽量保证本node内的CPU只访问本node内的内存块，那这样的效率就是最高的。
在运行程序的时候使用numactl -m和-physcpubind就能制定将这个程序运行在哪个cpu和哪个memory中。玩转cpu-topology给了一个表格，当程序只使用一个node资源和使用多个node资源的比较表（差不多是 38s 与 28s 的差距）。所以限定程序在numa node中运行是有实际意义的。
但是呢，话又说回来了，制定numa就一定好吗？--numa的陷阱。SWAP的罪与罚文章就说到了一个numa的陷阱的问题。现象是当你的服务器还有内存的时候，发现它已经在开始使用swap了，甚至已经导致机器出现停滞的现象。这个就有可能是由于numa的限制，如果一个进程限制它只能使用自己的numa节点的内存，那么当自身numa node内存使用光之后，就不会去使用其他numa node的内存了，会开始使用swap，甚至更糟的情况，机器没有设置swap的时候，可能会直接死机！所以你可以使用numactl --interleave=all来取消numa node的限制。
综上所述得出的结论就是，根据具体业务决定NUMA的使用。
如果你的程序是会占用大规模内存的，你大多应该选择关闭numa node的限制。因为这个时候你的程序很有几率会碰到numa陷阱。
另外，如果你的程序并不占用大内存，而是要求更快的程序运行时间。你大多应该选择限制只访问本numa node的方法来进行处理。
 转载：http://lib.csdn.net/article/linux/28546
 </description>
    </item>
    
    <item>
      <title>分布式文件系统-FastDFS</title>
      <link>http://zhaox.xyz/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-fastdfs/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-fastdfs/</guid>
      <description>一、FastDFS 简介  FastDFS是由国人余庆所开发，其项目地址：https://github.com/happyfish100 &amp;gt;FastDFS是一个轻量级的开源分布式文件系统，主要解决了大容量的文件存储和高并发访问的问题，文件存取时实现了负载均衡。 支持存储服务器在线扩容,支持相同的文件只保存一份,节约磁盘。 FastDFS只能通过 Client API 访问，不支持 POSIX 访问方式。 FastDFS 适合中大型网站使用，用来存储资源文件(如：图片、文档、视频等)
 二、FastDFS 组成部分及其它名词 1、tracker server 跟踪服务器：用来调度来自客户端的请求。且在内存中记录所有存储组和存储服务器的信息状态。
2、storage server 存储服务器：用来存储文件(data)和文件属性(metadata)
3、client 客户端：业务请求发起方，通过专用接口基于 TCP 协议与tracker以及storage server进行交互
group 组，也可称为卷：同组内上的文件是完全相同的
文件标识 包括两部分：组名和文件名(包含路径)
meta data 文件相关属性：键值对(Key Value Pair)方式
fid 文件标识符：
例如：
group1/M00/00/00/CgEOxVegXB2AdYafAAAB0b8tBbQ9155303
group_name：存储组的组名；上传完成后，需要客户端自行保存
M##：服务器配置的虚拟路径，与磁盘选项 store_path#对应两级以两位 16 进制数字命名的目录
文件名：与原文件名并不相同；由 storage server 根据特定信息生成。文件名包含：源存储服务器的 IP 地址、文件创建时间戳、文件大小、随机数和文件扩展名等
三、FastDFS 同步机制  1、同一组内的 storage server 之间是对等的，文件上传、删除等操作可以在任意一台 storage server 上进行； 2、文件同步只在同组内的 storage server 之间进行，采用 push 方式，即源服务器同步给目标服务器； 3、源头数据才需要同步，备份数据不需要再次同步，否则就构成环路了； 上述第二条规则有个例外，就是新增加一台 storage server 时，由已有的一台 storage server 将已有的所有数据（包括源头数据和备份数据）同步给该新增服务器。</description>
    </item>
    
    <item>
      <title>制作并安装FastDFS的RPM程序包</title>
      <link>http://zhaox.xyz/posts/%E5%88%B6%E4%BD%9C%E5%B9%B6%E5%AE%89%E8%A3%85fastdfs%E7%9A%84rpm%E7%A8%8B%E5%BA%8F%E5%8C%85/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/%E5%88%B6%E4%BD%9C%E5%B9%B6%E5%AE%89%E8%A3%85fastdfs%E7%9A%84rpm%E7%A8%8B%E5%BA%8F%E5%8C%85/</guid>
      <description>一、安装并制作 rpm 包 libfastcommon 注意:生成的 rpm 包全都在/root/rpmbuild/RPMS/下面，可以再其他主机直接使用
#!/bin/bash # Author: Antony # rpm build for fastdfs # Mail: zhaoxin@altamob.com,go80800@163.com # Desc: build libfastcommmon # 制作和安装libfastcommon cat &amp;lt;&amp;lt; EOF +-------------------------------------------+ | this is a rpmbuild libfastcommon script | | from Antony ....... | +-------------------------------------------- EOF # 制作源码文件 建议手动安装yum #yum groupinstall &amp;quot;Development Tools&amp;quot; &amp;quot;Server platform Development&amp;quot; -y cd /root/ git clone https://github.com/happyfish100/libfastcommon.git if [ $? -ne 0 ];then exit 1 fi libversion=$(grep -i &amp;quot;^version&amp;quot; libfastcommon/libfastcommon.</description>
    </item>
    
    <item>
      <title>Template Exmple</title>
      <link>http://zhaox.xyz/posts/template/</link>
      <pubDate>Sat, 01 Jan 2000 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/posts/template/</guid>
      <description>Content here</description>
    </item>
    
    <item>
      <title>Archive</title>
      <link>http://zhaox.xyz/archive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://zhaox.xyz/archive/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>